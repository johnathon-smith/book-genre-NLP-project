{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffcb45a",
   "metadata": {},
   "source": [
    "# Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18468ec7",
   "metadata": {},
   "source": [
    "For my MVP, I would like to use several algorithms with cross-validation and grid search. The algorithms I plan on using are:\n",
    "\n",
    "* Random Forest Classifier\n",
    "* K-Nearest Neighbors\n",
    "* Gaussian Naive Bayes\n",
    "* Multinomial Naive Bayes\n",
    "* XGBoost (If I can get it to work. I will attempt this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f0bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7083e",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1e83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurbs = pd.read_csv('blurbs_for_exploration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a3f7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>sub-genre</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lem_char_count</th>\n",
       "      <th>lem_word_count</th>\n",
       "      <th>lem_unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_words_per_sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>word_stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Horror</td>\n",
       "      <td>ghost-stories</td>\n",
       "      <td>Designed to appeal to the book lover, the Macm...</td>\n",
       "      <td>designed appeal book lover macmillan collector...</td>\n",
       "      <td>design appeal book lover macmillan collector '...</td>\n",
       "      <td>designed appeal book lover macmillan collector...</td>\n",
       "      <td>1102</td>\n",
       "      <td>147</td>\n",
       "      <td>120</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.9582</td>\n",
       "      <td>96</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Horror</td>\n",
       "      <td>ghost-stories</td>\n",
       "      <td>Part of the Penguin Orange Collection, a limit...</td>\n",
       "      <td>part penguin orange collection limitedrun seri...</td>\n",
       "      <td>part penguin orang collect limitedrun seri twe...</td>\n",
       "      <td>part penguin orange collection limitedrun seri...</td>\n",
       "      <td>954</td>\n",
       "      <td>118</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>55</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Horror</td>\n",
       "      <td>ghost-stories</td>\n",
       "      <td>Part of a new six-volume series of the best in...</td>\n",
       "      <td>part new sixvolume series best classic horror ...</td>\n",
       "      <td>part new sixvolum seri best classic horror sel...</td>\n",
       "      <td>part new sixvolume series best classic horror ...</td>\n",
       "      <td>1260</td>\n",
       "      <td>173</td>\n",
       "      <td>138</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.2144</td>\n",
       "      <td>85</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Horror</td>\n",
       "      <td>ghost-stories</td>\n",
       "      <td>A USA TODAY BESTSELLER!An Indie Next Pick!An O...</td>\n",
       "      <td>usa today bestselleran indie next pickan octob...</td>\n",
       "      <td>usa today bestselleran indi next pickan octob ...</td>\n",
       "      <td>usa today bestselleran indie next pickan octob...</td>\n",
       "      <td>800</td>\n",
       "      <td>104</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>-0.9530</td>\n",
       "      <td>63</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Horror</td>\n",
       "      <td>ghost-stories</td>\n",
       "      <td>From the New York Times best-selling author of...</td>\n",
       "      <td>new york times bestselling author southern boo...</td>\n",
       "      <td>new york time bestsel author southern book clu...</td>\n",
       "      <td>new york time bestselling author southern book...</td>\n",
       "      <td>603</td>\n",
       "      <td>77</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.9726</td>\n",
       "      <td>28</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    genre      sub-genre                                           original  \\\n",
       "0  Horror  ghost-stories  Designed to appeal to the book lover, the Macm...   \n",
       "1  Horror  ghost-stories  Part of the Penguin Orange Collection, a limit...   \n",
       "2  Horror  ghost-stories  Part of a new six-volume series of the best in...   \n",
       "3  Horror  ghost-stories  A USA TODAY BESTSELLER!An Indie Next Pick!An O...   \n",
       "4  Horror  ghost-stories  From the New York Times best-selling author of...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  designed appeal book lover macmillan collector...   \n",
       "1  part penguin orange collection limitedrun seri...   \n",
       "2  part new sixvolume series best classic horror ...   \n",
       "3  usa today bestselleran indie next pickan octob...   \n",
       "4  new york times bestselling author southern boo...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  design appeal book lover macmillan collector '...   \n",
       "1  part penguin orang collect limitedrun seri twe...   \n",
       "2  part new sixvolum seri best classic horror sel...   \n",
       "3  usa today bestselleran indi next pickan octob ...   \n",
       "4  new york time bestsel author southern book clu...   \n",
       "\n",
       "                                          lemmatized  lem_char_count  \\\n",
       "0  designed appeal book lover macmillan collector...            1102   \n",
       "1  part penguin orange collection limitedrun seri...             954   \n",
       "2  part new sixvolume series best classic horror ...            1260   \n",
       "3  usa today bestselleran indie next pickan octob...             800   \n",
       "4  new york time bestselling author southern book...             603   \n",
       "\n",
       "   lem_word_count  lem_unique_word_count  sentence_count  \\\n",
       "0             147                    120               8   \n",
       "1             118                     87               2   \n",
       "2             173                    138               7   \n",
       "3             104                     92               2   \n",
       "4              77                     74               5   \n",
       "\n",
       "   avg_words_per_sentence  sentiment  stopword_count  word_stopword_ratio  \n",
       "0                      18     0.9582              96                 0.65  \n",
       "1                      59     0.9100              55                 0.47  \n",
       "2                      25    -0.2144              85                 0.49  \n",
       "3                      52    -0.9530              63                 0.61  \n",
       "4                      15    -0.9726              28                 0.36  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blurbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd0163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21414 entries, 0 to 21413\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   genre                   21414 non-null  object \n",
      " 1   sub-genre               21414 non-null  object \n",
      " 2   original                21414 non-null  object \n",
      " 3   clean                   21414 non-null  object \n",
      " 4   stemmed                 21414 non-null  object \n",
      " 5   lemmatized              21414 non-null  object \n",
      " 6   lem_char_count          21414 non-null  int64  \n",
      " 7   lem_word_count          21414 non-null  int64  \n",
      " 8   lem_unique_word_count   21414 non-null  int64  \n",
      " 9   sentence_count          21414 non-null  int64  \n",
      " 10  avg_words_per_sentence  21414 non-null  int64  \n",
      " 11  sentiment               21414 non-null  float64\n",
      " 12  stopword_count          21414 non-null  int64  \n",
      " 13  word_stopword_ratio     21414 non-null  float64\n",
      "dtypes: float64(2), int64(6), object(6)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "blurbs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae24c6d",
   "metadata": {},
   "source": [
    "# Split the Data\n",
    "\n",
    "Since I will be using cross-validation, I won't need a validation set. Only train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e9655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16060, 14), (5354, 14))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(blurbs, stratify = blurbs.genre, test_size = .25, random_state = 123)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75ec1e",
   "metadata": {},
   "source": [
    "# Create X and y Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f975c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.drop(columns = ['genre']), train.genre\n",
    "X_test, y_test = test.drop(columns = ['genre']), test.genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65613cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16060, 13), (16060,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check train shapes\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8244671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5354, 13), (5354,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check test shapes\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502a2c9",
   "metadata": {},
   "source": [
    "# Scale the X Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2293f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Fit and transform the data on train \n",
    "X_train[['lem_char_count', 'lem_word_count', 'lem_unique_word_count', 'sentence_count', 'avg_words_per_sentence', 'sentiment', 'stopword_count', 'word_stopword_ratio']] = scaler.fit_transform(X_train[['lem_char_count', 'lem_word_count', 'lem_unique_word_count', 'sentence_count', 'avg_words_per_sentence', 'sentiment', 'stopword_count', 'word_stopword_ratio']])\n",
    "\n",
    "#Transform the data on test\n",
    "X_test[['lem_char_count', 'lem_word_count', 'lem_unique_word_count', 'sentence_count', 'avg_words_per_sentence', 'sentiment', 'stopword_count', 'word_stopword_ratio']] = scaler.transform(X_test[['lem_char_count', 'lem_word_count', 'lem_unique_word_count', 'sentence_count', 'avg_words_per_sentence', 'sentiment', 'stopword_count', 'word_stopword_ratio']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be75f0",
   "metadata": {},
   "source": [
    "# Part 1 - Engineered Features Only\n",
    "\n",
    "Make predictions using only the engineered features, not the term frequencies or TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892329f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the engineered features\n",
    "X_train_part1 = X_train.drop(columns = ['sub-genre', 'original', 'clean', 'stemmed', 'lemmatized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "818737a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lem_char_count</th>\n",
       "      <th>lem_word_count</th>\n",
       "      <th>lem_unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_words_per_sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>word_stopword_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17405</th>\n",
       "      <td>0.063722</td>\n",
       "      <td>0.070363</td>\n",
       "      <td>0.107901</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>0.132867</td>\n",
       "      <td>0.986747</td>\n",
       "      <td>0.048842</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11323</th>\n",
       "      <td>0.037368</td>\n",
       "      <td>0.041007</td>\n",
       "      <td>0.068819</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.878720</td>\n",
       "      <td>0.030056</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18975</th>\n",
       "      <td>0.053889</td>\n",
       "      <td>0.058248</td>\n",
       "      <td>0.100255</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>0.146853</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.047589</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8934</th>\n",
       "      <td>0.050461</td>\n",
       "      <td>0.060112</td>\n",
       "      <td>0.092608</td>\n",
       "      <td>0.025105</td>\n",
       "      <td>0.132867</td>\n",
       "      <td>0.014904</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.054450</td>\n",
       "      <td>0.064772</td>\n",
       "      <td>0.105353</td>\n",
       "      <td>0.029289</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>0.071384</td>\n",
       "      <td>0.405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lem_char_count  lem_word_count  lem_unique_word_count  sentence_count  \\\n",
       "17405        0.063722        0.070363               0.107901        0.029289   \n",
       "11323        0.037368        0.041007               0.068819        0.037657   \n",
       "18975        0.053889        0.058248               0.100255        0.020921   \n",
       "8934         0.050461        0.060112               0.092608        0.025105   \n",
       "594          0.054450        0.064772               0.105353        0.029289   \n",
       "\n",
       "       avg_words_per_sentence  sentiment  stopword_count  word_stopword_ratio  \n",
       "17405                0.132867   0.986747        0.048842                0.255  \n",
       "11323                0.062937   0.878720        0.030056                0.270  \n",
       "18975                0.146853   0.023256        0.047589                0.300  \n",
       "8934                 0.132867   0.014904        0.053225                0.325  \n",
       "594                  0.125874   0.600100        0.071384                0.405  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_part1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afc5d2",
   "metadata": {},
   "source": [
    "### Create Baseline\n",
    "\n",
    "I will use the dummy classifier with a stratify strategy to create my baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "193479f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27017434620174346"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate the model\n",
    "baseline_model = DummyClassifier(strategy = 'stratified', random_state = 123)\n",
    "\n",
    "#Fit the model\n",
    "baseline_model.fit(X_train_part1, y_train)\n",
    "\n",
    "#Score the model\n",
    "baseline_model.score(X_train_part1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93d312",
   "metadata": {},
   "source": [
    "__Baseline Accuracy: 27%__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1d7d1",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "Begin with the random forest classifier algorithm. Write a function that utilizes grid search and cross-validation to optimize it and return the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bad9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_models(X_train, y_train, param_dict, cv = 5):\n",
    "    \"\"\"\n",
    "    This function creates and returns an optimized random forest classification model. It also\n",
    "    prints out the best model's mean cross-validated accuracy score and parameters.\n",
    "    \n",
    "    This function takes in the X and y training sets to fit the models.\n",
    "    \n",
    "    This function takes in a dictionary that contains the parameters to be iterated through.\n",
    "    \n",
    "    This function also takes in a value for the number of cross validation folds to do.\n",
    "    The cv value defaults to 5.\n",
    "    \"\"\"\n",
    "    #Create the classifier model\n",
    "    clf = RandomForestClassifier(random_state = 123)\n",
    "    \n",
    "    #Create the GridSearchCV object\n",
    "    grid = GridSearchCV(clf, param_dict, cv = cv)\n",
    "    \n",
    "    #Fit the GridSearchCV object\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    #Print the best model's score and parameters\n",
    "    print('Mean Cross-Validated Accuracy: ', round(grid.best_score_, 4))\n",
    "    print('Max Depth: ', grid.best_params_['max_depth'])\n",
    "    print('Min Samples Per Leaf: ', grid.best_params_['min_samples_leaf'])\n",
    "    \n",
    "    #Return the best model\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5871f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dictionary of parameters and their values to iterate through\n",
    "rf_dict = {\n",
    "    'max_depth': range(14, 26),\n",
    "    'min_samples_leaf': range(1, 16)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e60068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_rf_model = get_random_forest_models(X_train_part1, y_train, rf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea0a86",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "\n",
    "- Mean Cross-Validated Accuracy: 0.4351\n",
    "- Max Depth: 14\n",
    "- Min Samples Per Leaf: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fee507",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "Now write a function to create an optimized K-Nearest Neighbors model. It will behave like the previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee3350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KNN_models(X_train_scaled, y_train, param_dict, cv = 5):\n",
    "    \"\"\"\n",
    "    This function takes in scaled data and builds an optimized KNN classification model. \n",
    "    It will use the parameters specified in the param_dict to optimizie across.\n",
    "    \n",
    "    This function utilizes GridSearchCV.\n",
    "    \n",
    "    This function returns the best model and prints out its parameters and mean\n",
    "    cross-validated accuracy.\n",
    "    \"\"\"\n",
    "    #Create the KNN model\n",
    "    clf = KNeighborsClassifier()\n",
    "    \n",
    "    #Create the GridSearchCV object\n",
    "    grid = GridSearchCV(clf, param_dict, cv = cv)\n",
    "    \n",
    "    #Fit the GridSearchCV object\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    #Print the best model's score and parameters\n",
    "    print('Mean Cross-Validated Accuracy: ', round(grid.best_score_, 4))\n",
    "    print('Num Neighbors: ', grid.best_params_['n_neighbors'])\n",
    "    print('Weights: ', grid.best_params_['weights'])\n",
    "    \n",
    "    #Return the best model\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f778ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dict of parameters and their values to optimize across\n",
    "knn_dict = {\n",
    "    'n_neighbors': range(10, 500, 10),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb03c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_knn_model = get_KNN_models(X_train_part1, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a74b91b",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "\n",
    "- Mean Cross-Validated Accuracy: 0.4215\n",
    "- Num Neighbors: 130\n",
    "- Weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318fa77",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Write a function that creates a Gaussian Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48fac16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gauss_models(X_train_scaled, y_train, param_dict, cv = 5):\n",
    "    \"\"\"\n",
    "    This function will create an optimized Gaussian Naive Bayes classification model. It will\n",
    "    use the X_train_scaled and y_train data for fitting. The param_dict contains the parameters\n",
    "    and their values that the GridSearchCV function will optimize across. The cv parameter\n",
    "    indicates how many folds will be fitted and evaluated, and defaults to 5.\n",
    "    \n",
    "    This function prints out the mean cross-validated accuracy, best paramters, and returns\n",
    "    the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create the Gaussian Naive Bayes model\n",
    "    clf = GaussianNB()\n",
    "    \n",
    "    #Create the GridSearchCV object\n",
    "    grid = GridSearchCV(clf, param_dict, cv = cv)\n",
    "    \n",
    "    #Fit the GridSearchCV object\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    #Print the best model's score and parameters\n",
    "    print('Mean Cross-Validated Accuracy: ', round(grid.best_score_, 4))\n",
    "    print('Smoothing: ', grid.best_params_['var_smoothing'])\n",
    "    \n",
    "    #Return the best model\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23419337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the param_dict\n",
    "gauss_dict = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81dfdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_gauss_model = get_gauss_models(X_train_part1, y_train, gauss_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03305e3d",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "\n",
    "- Mean Cross-Validated Accuracy: 0.3993\n",
    "- Smoothing: 0.035"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c3c4b",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Write a function that creates a multinomial naive bayes model. I know this function is better suited to event driven counts, but I want to see if it will work using the engineered features in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c68ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multinomial_models(X_train_scaled, y_train, param_dict, cv = 5):\n",
    "    \"\"\"\n",
    "    This function will create an optimized Multinomial Naive Bayes classification model. It will\n",
    "    use the X_train_scaled and y_train data for fitting. The param_dict contains the parameters\n",
    "    and their values that the GridSearchCV function will optimize across. The cv parameter\n",
    "    indicates how many folds will be fitted and evaluated, and defaults to 5.\n",
    "    \n",
    "    This function prints out the mean cross-validated accuracy, best paramters, and returns\n",
    "    the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create the Multinomial Niave Bayes model\n",
    "    clf = MultinomialNB()\n",
    "    \n",
    "    #Create the GridSearchCV object\n",
    "    grid = GridSearchCV(clf, param_dict, cv = cv)\n",
    "    \n",
    "    #Fit the GridSearchCV object\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    #Print the best model's score and parameters\n",
    "    print('Mean Cross-Validated Accuracy: ', round(grid.best_score_, 4))\n",
    "    print('Alpha: ', grid.best_params_['alpha'])\n",
    "    print('Fit Prior: ', grid.best_params_['fit_prior'])\n",
    "    \n",
    "    #Return the best model\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a82b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the param_dict\n",
    "multinomial_dict = {\n",
    "    'alpha': range(1, 101),\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb62a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the multinomial models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_part1, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf537ac2",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "\n",
    "- Mean Cross-Validated Accuracy: 0.3217\n",
    "- Alpha: 1\n",
    "- Fit Prior: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f62307",
   "metadata": {},
   "source": [
    "# Part 2 - Word Counts Only\n",
    "\n",
    "Make predictions using only the word counts, not the engineered features or TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18d51b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the count vectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Create the bag of words from the lemmatized column in the X_train df\n",
    "X_train_bow = cv.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40b9832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize another count vectorizer and add bigrams\n",
    "cv = CountVectorizer(ngram_range = (1,2))\n",
    "\n",
    "#Create the bag of words from the lemmatized column in the X_train df\n",
    "X_train_bow_bigrams = cv.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b168ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize another count vectorizer and add trigrams\n",
    "cv = CountVectorizer(ngram_range = (1,3))\n",
    "\n",
    "#Create the bag of words from the lemmatized column in the X_train df\n",
    "X_train_bow_trigrams = cv.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9bc8cb",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Individual Words\n",
    "\n",
    "Create the Random Forest Classifier models using only the bag of words as features. Start with only the individual word counts, then add bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff87be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dict = {\n",
    "    'max_depth': range(41, 46),\n",
    "    'min_samples_leaf': range(1, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de64448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_rf_model = get_random_forest_models(X_train_bow, y_train, rf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b118c",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "\n",
    "- Mean Cross-Validated Accuracy: 0.8043\n",
    "- Max Depth: 43\n",
    "- Min Samples Per Leaf: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd36cb",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Individual Words and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db06386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dict = {\n",
    "    'max_depth': range(46, 51),\n",
    "    'min_samples_leaf': range(1,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8f08394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function to create the models\n",
    "#best_rf_model = get_random_forest_models(X_train_bow_bigrams, y_train, rf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0ca4f",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "\n",
    "- Mean Cross-Validated Accuracy: 0.7773\n",
    "- Max Depth: 50\n",
    "- Min Samples Per Leaf: 1\n",
    "\n",
    "Due to the long run times for each test, I'm stopping here. It doesn't seem to be better than the unigram only feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596771ff",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Individual Words, Bigrams, and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4df6772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dict = {\n",
    "    'max_depth': range(45, 51),\n",
    "    'min_samples_leaf': range(1, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7d7eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function to create the models\n",
    "#best_rf_model = get_random_forest_models(X_train_bow_trigrams, y_train, rf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f7f96",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.7468\n",
    "- Max Depth: 49\n",
    "- Min Samples Per Leaf: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258b6f2",
   "metadata": {},
   "source": [
    "Due to the long run times for each test, I'm stopping here. It doesn't seem to be better than the unigram only feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cbe4a",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor - Individual Words\n",
    "\n",
    "After doing some research, I have learned that KNN can actually be used for text classification. However, instead of just using simple word counts, I should use normalized TF-IDF values. I will still attempt the classification with just word counts, but the real test for this algorithm will be in the TF-IDF section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65fa5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dict of parameters and their values to optimize across\n",
    "knn_dict = {\n",
    "    'n_neighbors': range(11, 502, 10),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32575f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_knn_model = get_KNN_models(X_train_bow, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae229720",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.3832\n",
    "- Num Neighbors: 10\n",
    "- Weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc017141",
   "metadata": {},
   "source": [
    "Yeah, it seems like KNN won't be much use here. I'm going to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ecc43",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes - Individual Words\n",
    "\n",
    "After doing some research, I have learned that the Gaussian Naive Bayes algorithm is actually not a good choice for text classification. From here on, I will no longer consider this algorithm and instead focus on the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529e84a",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Individual Words\n",
    "\n",
    "After doing some research, I have learned that the Multinomial Naive Bayes algorithm is a good choice for text classification, especially when dealing with word counts. I do not know how it will perform with TF-IDF, but I will find out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfe2a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the param_dict\n",
    "multinomial_dict = {\n",
    "    'alpha': range(1, 101),\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e68f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_bow, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9bfe21",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8649\n",
    "- Alpha: 1\n",
    "- Fit Prior: True\n",
    "\n",
    "I'd also like to note that this algorithm was __extremely fast__ compared to the Random Forest Classifiers above. It also performed quite a bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626f617",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Individual Words and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cf1bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_bow_bigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ccf415",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "\n",
    "- Mean Cross-Validated Accuracy: 0.8786\n",
    "- Alpha: 1\n",
    "- Fit Prior: True\n",
    "\n",
    "Performed slightly better than just using the individual word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db1432",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Individual Words, Bigrams, and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b77149c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_bow_trigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648387c7",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8813\n",
    "- Alpha: 1\n",
    "- Fit Prior: True\n",
    "\n",
    "Performed slightly better than just using unigram and bigram counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dbb7cc",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Bigrams\n",
    "\n",
    "Since this algorithm is performing so well and training so quickly, I've decided to see how it will do using only Bigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0d763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the count vectorizer\n",
    "cv = CountVectorizer(ngram_range = (2,2))\n",
    "\n",
    "#Fit and transform the train data\n",
    "X_train_bigrams_only = cv.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c083816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_bigrams_only, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56bd58",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8311\n",
    "- Alpha: 5\n",
    "- Fit Prior: True\n",
    "\n",
    "Although it still performed well, it wasn't quite as good as the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e30619",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Trigrams\n",
    "\n",
    "Same as above, but with trigrams only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e319581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the count vectorizer\n",
    "cv = CountVectorizer(ngram_range = (3,3))\n",
    "\n",
    "#Fit and transform the train data\n",
    "X_train_trigrams_only = cv.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99220cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_trigrams_only, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5b8bd",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.6686\n",
    "- Alpha: 13\n",
    "- Fit Prior: True\n",
    "\n",
    "This one was surprisingly bad compared to the others. It seems using bigrams and trigrams on their own is not better than using them as supplemental data to the individual word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa63b6",
   "metadata": {},
   "source": [
    "# Part 3: TF-IDF\n",
    "\n",
    "Now I will build models using only the TF-IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d01024a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tf-idf vectorizer without bigrams or trigrams\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "104e6c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tf-idf vectorizer with unigrams and bigrams\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,2))\n",
    "\n",
    "X_train_tfidf_bigrams = tfidf.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f5986dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tf-idf vectorizer with unigrams, bigrams, and trigrams\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,3))\n",
    "\n",
    "X_train_tfidf_trigrams = tfidf.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d235e",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Unigrams Only\n",
    "\n",
    "Here, I will build a Random Forest Classifier model using the unigram tf-idf feature set. Since the random forest classifier took so long to run in the last section, I'm only going to run each of these once, using the same parameter dictionary that gave me the best results from before. It will give me a genreal idea of what I can expect out of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47ee0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dict = {\n",
    "    'max_depth': range(45, 51),\n",
    "    'min_samples_leaf': range(1, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cbd6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function to create the models\n",
    "#best_rf_model = get_random_forest_models(X_train_tfidf, y_train, rf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6558c7",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8104\n",
    "- Max Depth: 49\n",
    "- Min Samples Per Leaf: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18b6c0",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "390e1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function to create the models\n",
    "#best_rf_model = get_random_forest_models(X_train_tfidf_bigrams, y_train, rf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b52934",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.7773\n",
    "- Max Depth: 49\n",
    "- Min Samples Per Leaf: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c18657",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Unigrams, Bigrams, and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7550c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function to create the models\n",
    "#best_rf_model = get_random_forest_models(X_train_tfidf_trigrams, y_train, rf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145d6fe",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.7502\n",
    "- Max Depth: 50\n",
    "- Min Samples Per Leaf: 1\n",
    "\n",
    "Not quite as good as the the last model, but still not bad. It did take a while though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd20c9",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors - Unigrams Only\n",
    "\n",
    "Now create a KNN model that uses the unigram tf-idf feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20818c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dict of parameters and their values to optimize across\n",
    "knn_dict = {\n",
    "    'n_neighbors': range(11, 502, 10),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4d18bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_knn_model = get_KNN_models(X_train_tfidf, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d00958",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8286\n",
    "- Num Neighbors: 171\n",
    "- Weights: distance\n",
    "\n",
    "Pretty good! Way better than when we used KNN for the unigram counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08069b",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors - Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f757ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_knn_model = get_KNN_models(X_train_tfidf_bigrams, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a845e8",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8353\n",
    "- Num Neighbors: 251\n",
    "- Weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ed133",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors - Unigrams, Bigrams, and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d753393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_knn_model = get_KNN_models(X_train_tfidf_trigrams, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a12375",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8333\n",
    "- Num Neighbors: 101\n",
    "- Weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33030aaf",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams\n",
    "\n",
    "I know that multinomial naive bayes is really only supposed to work with integer counts, but I read in the sklearn documentation for the algorithm that fractional counts also work, specifically tf-idf. I'm not sure how it will perform, but I've read that multinomial naive bayes often does better with tf-idf than just a simple word count. Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2c0a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the param_dict\n",
    "multinomial_dict = {\n",
    "    'alpha': range(1, 101),\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f983e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_tfidf, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e5eda",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8302\n",
    "- Alpha: 1\n",
    "- Fit Prior: False\n",
    "\n",
    "Although not as good as model just using the simple word count, it still performed surprisingly well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a88e9",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "628cf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_tfidf_bigrams, y_train, multinomial_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa1211",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8373\n",
    "- Alpha: 1\n",
    "- Fit Prior: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ee98a",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams, Bigrams, and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7fed492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_tfidf_trigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d45fa0",
   "metadata": {},
   "source": [
    "# Part 4 - Word Count and TF-IDF\n",
    "\n",
    "Can we combine the word count feature set with the tf-idf feature set and get better results? I will use scipy.sparse.hstack to concatenate the sparse matrices and then feed the new sparse into a multinomial naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ceaee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1578396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the sparse matrices\n",
    "X_train_bow_tfidf = hstack([X_train_bow, X_train_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee950f3",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c96054cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the param_dict\n",
    "multinomial_dict = {\n",
    "    'alpha': range(1, 101),\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4e73ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_bow_tfidf, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1c48c",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8669\n",
    "- Alpha: 1\n",
    "- Fit Prior: True\n",
    "\n",
    "Performed slightly better than simply using the word counts for unigrams only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b84f25",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d2f4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the sparse matrices\n",
    "X_train_bow_tfidf_bigrams = hstack([X_train_bow_bigrams, X_train_tfidf_bigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "390e37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_bow_tfidf_bigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d9fb6",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.875\n",
    "- Alpha: 1\n",
    "- Fit Prior: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a659593",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams, Bigrams, and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddf8b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the sparse matrices\n",
    "X_train_bow_tfidf_trigrams = hstack([X_train_bow_trigrams, X_train_tfidf_trigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d330df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_bow_tfidf_trigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004fe539",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8786\n",
    "- Alpha: 1\n",
    "- Fit Prior: False\n",
    "\n",
    "Although I think this is the second best performing model I've created so far, it is not as good as the unigram, bigram, and trigram word count feature set. But only by about 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fdc22",
   "metadata": {},
   "source": [
    "# Part 5 - Combining Word Counts with Engineered Features\n",
    "\n",
    "In this section, I will attempt to combine the vectorized word counts with the engineered features from Part 1. I'm not sure if it will work, but I think it's worth a try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9bcdf8",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams with Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56b31994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the data\n",
    "X_train_part_5 = hstack([X_train_part1, X_train_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f375539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_part_5, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df050a",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8648\n",
    "- Alpha: 1\n",
    "- Fit Prior: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c976c",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams and Bigrams with Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "428b8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the data\n",
    "X_train_part_5_bigrams = hstack([X_train_part1, X_train_bow_bigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4103c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_part_5_bigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03631ead",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8784\n",
    "- Alpha: 1\n",
    "- Fit Prior: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdda5f9",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams, Bigrams, and Trigrams with Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79576dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the data\n",
    "X_train_part_5_trigrams = hstack([X_train_part1, X_train_bow_trigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "986324fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_part_5_trigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415748b",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8813\n",
    "- Alpha: 1\n",
    "- Fit Prior: True\n",
    "\n",
    "After reviewing the results, it appears that the engineered features made no difference in the mean accuracies of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613617c",
   "metadata": {},
   "source": [
    "# Part 6 - TF-IDF Combined with Engineered Features\n",
    "\n",
    "In this section, I will test whether or not the engineered features offer any value when combined with the TF-IDF feature sets. Since KNN performed relatively well with the TF-IDF feature sets, I will test it along with the multinomial naive bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cf0d7",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor - Unigrams and Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da4b0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the data sets\n",
    "X_train_part_6 = hstack([X_train_tfidf, X_train_part1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6e52dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dict of parameters and their values to optimize across\n",
    "knn_dict = {\n",
    "    'n_neighbors': range(11, 502, 10),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27feb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the models\n",
    "#best_knn_model = get_KNN_models(X_train_part_6, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d24017",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8007\n",
    "- Num Neighbors: 31\n",
    "- Weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9b120",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors - Unigrams, Bigrams and Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c26c7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the data sets\n",
    "X_train_part_6_bigrams = hstack([X_train_tfidf_bigrams, X_train_part1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f618eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validated Accuracy:  0.803\n",
      "Num Neighbors:  11\n",
      "Weights:  distance\n"
     ]
    }
   ],
   "source": [
    "#Create the models\n",
    "#best_knn_model = get_KNN_models(X_train_part_6_bigrams, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d68baa",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.803\n",
    "- Num Neighbors: 11\n",
    "- Weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a5a7a",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors - Unigrams, Bigrams, Trigrams and Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18a81218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the data sets\n",
    "X_train_part_6_trigrams = hstack([X_train_tfidf_trigrams, X_train_part1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3de89e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validated Accuracy:  0.7958\n",
      "Num Neighbors:  11\n",
      "Weights:  distance\n"
     ]
    }
   ],
   "source": [
    "#Create the models\n",
    "#best_knn_model = get_KNN_models(X_train_part_6_trigrams, y_train, knn_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a49b3e",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.7958\n",
    "- Num Neighbors: 11\n",
    "- Weights: distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcca1c8",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams and Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea778974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the param_dict\n",
    "multinomial_dict = {\n",
    "    'alpha': range(1, 101),\n",
    "    'fit_prior': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad78c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validated Accuracy:  0.8085\n",
      "Alpha:  1\n",
      "Fit Prior:  False\n"
     ]
    }
   ],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_part_6, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002da139",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.8085\n",
    "- Alpha: 1\n",
    "- Fit Prior: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e11346",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams, Bigrams and Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23f7bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validated Accuracy:  0.7575\n",
      "Alpha:  1\n",
      "Fit Prior:  False\n"
     ]
    }
   ],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_part_6_bigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1bd13",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.7575\n",
    "- Alpha: 1\n",
    "- Fit Prior: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0253b21a",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes - Unigrams, Bigrams, Trigrams and Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecadb71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross-Validated Accuracy:  0.714\n",
      "Alpha:  1\n",
      "Fit Prior:  False\n"
     ]
    }
   ],
   "source": [
    "#Create the models\n",
    "#best_multinomial_model = get_multinomial_models(X_train_part_6_trigrams, y_train, multinomial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc613e",
   "metadata": {},
   "source": [
    "Best Attempt:\n",
    "    \n",
    "- Mean Cross-Validated Accuracy: 0.714\n",
    "- Alpha: 1\n",
    "- Fit Prior: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c4a96",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After creating all of these different models and testing the different feature sets, I have found that my best model was the Multinomial Naive Bayes algorithm with the vectorized unigram, bigram, and trigram count feature set. If I want to improve the accuracy of my model, I will need to build better engineered features or improve the quality of my data. I could also try rerunnning everything above with the stemmed descriptions instead of the lemmatized ones. From here, I will move on to the final report.\n",
    "\n",
    "Best Model:\n",
    "\n",
    "- Algorithm: Multinomial Naive Bayes\n",
    "- Mean Cross-Validated Accuracy: .8813\n",
    "- Alpha: 1\n",
    "- Fit Prior: True\n",
    "- Feature Set: Unigram, Bigram, and Trigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee55c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
